#!/bin/bash
#
# run-vm
#
# (C) Stephen Bates <sbates@raithlin>
#
# A simple script to run a VM that was (probably) generated using the
# gen-vm script. Note that to pass in a host filesystem you need to
# run something like the following (in the guest):
#
# sudo mount -t 9p -o trans=virtio,version=9p2000.L hostfs \
#   /home/batesste/Projects
#
# For some reason -t virtfs does not work when running qemu directly
# but it does work when running via libvirt. Go figure! You can also
# add this to /etc/fstab in the guest for a permenant solution.
#
# hostfs /home/batesste/Projects 9p trans=virtio,version=9p2000.L,nofail 0 1
#
# Change NVME in order to add emulated NVMe SSD(s) to the VM. There
# are three modes for this:
#   1. If you specify an unsigned number the script checks to see if
#      image file(s) exist and then creates that many emulated NVMe
#      SSDs with pretty reasonable arguments.
#   2. If you specify a string then we use this literally as the
#      argument  string given at the command line or I
#   3. If you specify "true" we implement a simple null_blk backed
#      NVMe drive. You can insert a null_blk device into the kernel of
#      the host using a command like:
#        sudo modprobe null_blk queue_mode=2 gb=1024
#
# Note you can pass in multiple PCIe devices from the host using
# PCI_HOSTDEV where it is equal to the host PCI bus addresses separated
# by commas. The devices must be unbound from their native host driver
# and assigned to vfio-pci.
#
# NVME_TRACE enables tracing of NVMe device operations. Options are:
#   - "doorbell": Trace submission/completion queue doorbell writes
#   - "all": Trace all NVMe events
#   - Custom trace event name (e.g., "pci_nvme_read")
# Use NVME_TRACE_FILE to redirect trace output to a file instead of
# stderr.
#
# Post 10.1.0 versions of QEMU have libvfio-user support which allows
# us to create emulated PCIe devices via a socket and the API provided
# by that library. You can specify these device(s) via a comma-sperated
# list of sockets in VFIO_USERDEV.
#
# Set PCI_MMIO_BRIDGE to true to enable the out-of-tree
# PCI_MMIO_BRIDGE. Note this needs NVMe shadow doorbells to be
# disabled in the emulated NVMe devices.

QEMU_PATH=${QEMU_PATH:-}
VM_NAME=${VM_NAME:-qemu-minimal}
ARCH=${ARCH:-amd64}
VCPUS=${VCPUS:-2}
VMEM=${VMEM:-4096M}
FILESYSTEM=${FILESYSTEM:-none}
IMAGES=${IMAGES:-../images}
SSH_PORT=${SSH_PORT:-2222}
KVM=${KVM:-enable}
NVME=${NVME:-none}
NVME_TRACE=${NVME_TRACE:-none}
NVME_TRACE_FILE=${NVME_TRACE_FILE:-}
PCI_TESTDEV=${PCI_TESTDEV:-none}
PCI_HOSTDEV=${PCI_HOSTDEV:-none}
VFIO_USERDEV=${VFIO_USERDEV:-none}
PCI_MMIO_BRIDGE=${PCI_MMIO_BRIDGE:-none}

NVME_SIZE=1024G

if [ ${KVM} == "enable" ]; then
    KVM=",accel=kvm"
else
    KVM=""
fi

if [ ${FILESYSTEM} == "none" ]; then
    FILESYSTEM_ARGS=""
else
    FILESYSTEM_ARGS="-object memory-backend-memfd,id=mem0,size=2G -virtfs "
    FILESYSTEM_ARGS+="local,path=${FILESYSTEM},security_model=passthrough,"
    FILESYSTEM_ARGS+="mount_tag=hostfs "
fi

if [ ${ARCH} == "amd64" ]; then
    QARCH="x86_64"
    QARCH_ARGS="-machine q35${KVM} -cpu EPYC"
elif [ ${ARCH} == "arm64" ]; then
    QARCH="aarch64"
    QARCH_ARGS="-machine virt,gic-version=max${KVM} -cpu max -bios "
    QARCH_ARGS+="/usr/share/qemu-efi-aarch64/QEMU_EFI.fd"
elif [ ${ARCH} == "riscv64" ]; then
    QARCH="riscv64"
    QARCH_ARGS="-machine virt,${KVM} -kernel "
    QARCH_ARGS+="/usr/lib/u-boot/qemu-riscv64_smode/uboot.elf"
else
    echo "Error: No ARCH mapping exists for ${ARCH}! Exiting."; exit 1
fi

function nvme_create {
    if [ ! -f ${IMAGES}/${1}.qcow2 ]; then
        qemu-img create -f qcow2 ${IMAGES}/${1}.qcow2 $2 >> /dev/null
    fi
    echo "-drive file=${IMAGES}/${1}.qcow2,format=qcow2,if=none,id=nvme-${3} "\
         "-device nvme,serial=${1},drive=nvme-${3},ioeventfd=off,dbcs=off "
}

  # Check that the user has provided valid PCIe bus addresses and also check
  # that the devices associated with these addresses are bound to vfio-pci
function pci_check () {
    if [[ ! ${1} =~ ^([0-9a-fA-F]{4}:)?[0-9a-fA-F]{2}:[0-9a-fA-F]{2}\.[0-9]$ ]]; then
        echo "ERROR: PCIe bus address is invalid (${1})."
        exit 1
    fi

    lspci -k -s ${1} | grep "vfio-pci" > /dev/null 2>&1
    if [ $? -eq 1 ]; then
        echo "ERROR: Device ${1} is not bound to vfio-pci driver."
        exit 1
    fi
}

# Setup NVMe tracing if requested
TRACE_ARGS=""
if [ ${NVME_TRACE} != "none" ]; then
    if [ ${NVME_TRACE} == "doorbell" ]; then
        TRACE_ARGS="-trace enable=pci_nvme_mmio_doorbell_sq "
        TRACE_ARGS+="-trace enable=pci_nvme_mmio_doorbell_cq "
    elif [ ${NVME_TRACE} == "all" ]; then
        TRACE_ARGS="-trace enable=pci_nvme* "
    else
        # User specified custom trace events
        TRACE_ARGS="-trace enable=${NVME_TRACE} "
    fi

    # Optionally write to a file instead of stderr
    if [ ! -z "${NVME_TRACE_FILE}" ]; then
        TRACE_ARGS+="-trace file=${NVME_TRACE_FILE} "
    fi
fi

if [ ${NVME} == "none" ]; then
    NVME_ARGS=""
elif [ ${NVME} == "true" ]; then
    NVME_ARGS=$(echo "-drive file=/dev/nullb0,format=raw,if=none,id=nvme-1 "\
                     "-device nvme,serial=${VM_NAME}-nvme1,drive=nvme-1,")
elif [[ ${NVME} =~ ^[0-9]+$ ]]; then
    NVME_ARGS=""
    for i in $(seq 1 ${NVME}); do
        NVME_NAME=${VM_NAME}-nvme${i}
        NVME_ARGS+=$(nvme_create ${NVME_NAME} ${NVME_SIZE} ${i})
    done
else
    NVME_ARGS=${NVME}
fi

if [ ${PCI_TESTDEV} == "none" ]; then
    PCI_TESTDEV_ARGS=""
else
    PCI_TESTDEV_ARGS="-device pci-testdev,membar=16G,membar-backed=true"
fi

if [ ${PCI_HOSTDEV} == "none" ]; then
    PCI_HOSTDEV_ARGS=""
else
    for THIS_PCI_HOSTDEV in ${PCI_HOSTDEV//,/ }; do
        pci_check ${THIS_PCI_HOSTDEV}
         PCI_HOSTDEV_ARGS+="-device vfio-pci,host=${THIS_PCI_HOSTDEV} "
    done
fi

if [ ${VFIO_USERDEV} == "none" ]; then
    VFIO_USERDEV_ARGS=()
else
    # When using vfio-user, we need ALL guest RAM to be shareable via a memfd
    VFIO_USERDEV_ARGS+=( -object memory-backend-memfd,id=mem-vfio-user,size=${VMEM},share=on  )
    VFIO_USERDEV_ARGS+=( -numa node,memdev=mem-vfio-user )
    for THIS_VFIO_USERDEV in ${VFIO_USERDEV//,/ }; do
        if [ ! -S ${THIS_VFIO_USERDEV} ]; then
            echo "ERROR: Socket ${THIS_VFIO_USERDEV} does not exist."
            exit 1
        fi
        DEV_JSON=$(printf '{"driver":"vfio-user-pci","socket":{"path":"%s","type":"unix"}}' \
            ${THIS_VFIO_USERDEV})
        VFIO_USERDEV_ARGS+=( -device "${DEV_JSON}" )
    done
fi

if [ ${PCI_MMIO_BRIDGE} == "none" ]; then
    PCI_MMIO_BRIDGE_ARGS=""
else
    echo "WARNING: PCI_MMIO_BRIDGE: Make sure emulated NVMe devices have ioeventfd=off,dbcs=off!"
    PCI_MMIO_BRIDGE_ARGS="-device pci-mmio-bridge,id=mmio-bridge,"
    PCI_MMIO_BRIDGE_ARGS+="shadow-gpa=0x80000000,shadow-size=8192,"
    PCI_MMIO_BRIDGE_ARGS+="poll-interval-ns=1000000,addr=8.0"
    PCI_MMIO_BRIDGE_ARGS+=" -trace pci_mmio_*"
fi

${QEMU_PATH}qemu-system-${QARCH} \
   ${QARCH_ARGS} \
   ${TRACE_ARGS} \
   -smp cpus=${VCPUS} \
   -m ${VMEM} \
   ${FILESYSTEM_ARGS} \
   -nographic \
   ${PCIE_ROOT_PORTS} \
   ${NVME_ARGS} \
   ${PCI_TESTDEV_ARGS} \
   ${PCI_HOSTDEV_ARGS} \
   ${PCI_MMIO_BRIDGE_ARGS} \
   "${VFIO_USERDEV_ARGS[@]}" \
   -drive if=virtio,format=qcow2,file=${IMAGES}/${VM_NAME}.qcow2 \
   -netdev user,id=net0,hostfwd=tcp::${SSH_PORT}-:22 \
   -device virtio-net-pci,netdev=net0
