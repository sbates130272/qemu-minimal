#!/bin/bash
#
# run-vm
#
# (C) Stephen Bates <sbates@raithlin>
#
# A simple script to run a VM that was (probably) generated using the
# gen-vm script. Note that to pass in a host filesystem you need to
# run something like the following (in the guest):
#
# sudo mount -t 9p -o trans=virtio,version=9p2000.L hostfs \
#   /home/batesste/Projects
#
# For some reason -t virtfs does not work when running qemu directly
# but it does work when running via libvirt. Go figure! You can also
# add this to /etc/fstab in the guest for a permenant solution.
#
# hostfs /home/batesste/Projects 9p trans=virtio,version=9p2000.L 0 1
#
# Change NVME in order to add emulated NVMe SSD(s) to the VM. There
# are three modes for this:
#   1. If you specify an unsigned number the script checks to see if
#      image file(s) exist and then creates that many emulated NVMe
#      SSDs with pretty reasonable arguments.
#   2. If you specify a string then we use this literally as the
#      argument  string given at the command line or I
#   3. If you specify "true" we implement a simple null_blk backed
#      NVMe drive. You can insert a null_blk device into the kernel of
#      the host using a command like:
#        sudo modprobe null_blk queue_mode=2 gb=1024
#
# Note you can pass in multiple PCIe devices from the host using
# PCI_HOSTDEV where it is equal to the host PCI bus addresses separated
# by commas. The devices must be unbound from their native host driver
# and assigned to vfio-pci.

QEMU_PATH=${QEMU_PATH:-}
VM_NAME=${VM_NAME:-qemu-minimal}
ARCH=${ARCH:-amd64}
VCPUS=${VCPUS:-2}
VMEM=${VMEM:-4096}
FILESYSTEM=${FILESYSTEM:-none}
IMAGES=${IMAGES:-../images}
SSH_PORT=${SSH_PORT:-2222}
KVM=${KVM:-enable}
NVME=${NVME:-none}
PCI_TESTDEV=${PCI_TESTDEV:-none}
PCI_HOSTDEV=${PCI_HOSTDEV:-none}

NVME_SIZE=1024G

if [ ${KVM} == "enable" ]; then
    KVM=",accel=kvm"
else
    KVM=""
fi

if [ ${FILESYSTEM} == "none" ]; then
    FILESYSTEM_ARGS=""
else
    FILESYSTEM_ARGS="-object memory-backend-memfd,id=mem,size=2G -virtfs local,path=${FILESYSTEM},security_model=passthrough,mount_tag=hostfs "
fi

if [ ${ARCH} == "amd64" ]; then
    QARCH="x86_64"
    QARCH_ARGS="-machine q35${KVM} -cpu EPYC"
elif [ ${ARCH} == "arm64" ]; then
    QARCH="aarch64"
    QARCH_ARGS="-machine virt,gic-version=max${KVM} -cpu max -bios /usr/share/qemu-efi-aarch64/QEMU_EFI.fd"
elif [ ${ARCH} == "riscv64" ]; then
    QARCH="riscv64"
    QARCH_ARGS="-machine virt,${KVM} -kernel /usr/lib/u-boot/qemu-riscv64_smode/uboot.elf"
else
    echo "Error: No ARCH mapping exists for ${ARCH}! Exiting."; exit -1
fi

function nvme_create {
    if [ ! -f ${IMAGES}/${1}.qcow2 ]; then
        qemu-img create -f qcow2 ${IMAGES}/${1}.qcow2 $2 >> /dev/null
    fi
    echo "-drive file=${IMAGES}/${1}.qcow2,format=qcow2,if=none,id=nvme-${3}"\
         "-device nvme,serial=${1},drive=nvme-${3} "
}

  # Check that the user has provided valid PCIe bus addresses and also check
  # that the devices associated with these addresses are bound to vfio-pci
function pci_check () {
    if [[ ! ${1} =~ ^([0-9a-fA-F]{4}:)?[0-9a-fA-F]{2}:[0-9a-fA-F]{2}\.[0-9]$ ]]; then
        echo "ERROR: PCIe bus address is invalid (${1})."
        exit 1
    fi

    lspci -k -s ${1} | grep "vfio-pci" > /dev/null 2>&1
    if [ $? -eq 1 ]; then
        echo "ERROR: Device ${1} is not bound to vfio-pci driver."
        exit 1
    fi
}

if [ ${NVME} == "none" ]; then
    NVME_ARGS=""
elif [ ${NVME} == "true" ]; then
    NVME_ARGS=$(echo "-drive file=/dev/nullb0,format=raw,if=none,id=nvme-1"\
                     "-device nvme,serial=${VM_NAME}-nvme1,drive=nvme-1 ")
elif [[ ${NVME} =~ ^[0-9]+$ ]]; then
    NVME_ARGS=""
    for i in $(seq 1 ${NVME}); do
        NVME_NAME=${VM_NAME}-nvme${i}
        NVME_ARGS+=$(nvme_create ${NVME_NAME} ${NVME_SIZE} ${i})
    done
else
    NVME_ARGS=${NVME}
fi

if [ ${PCI_TESTDEV} == "none" ]; then
    PCI_TESTDEV_ARGS=""
else
    PCI_TESTDEV_ARGS="-device pci-testdev,membar=16G,membar-backed=true"
fi

if [ ${PCI_HOSTDEV} == "none" ]; then
    PCI_HOSTDEV_ARGS=""
else
    for THIS_PCI_HOSTDEV in ${PCI_HOSTDEV//,/ }; do
        pci_check ${THIS_PCI_HOSTDEV}
        PCI_HOSTDEV_ARGS+="-device vfio-pci,host=${THIS_PCI_HOSTDEV} "
    done
fi

${QEMU_PATH}qemu-system-${QARCH} \
   ${QARCH_ARGS} \
   -smp cpus=${VCPUS} \
   -m ${VMEM} \
   ${FILESYSTEM_ARGS} \
   -nographic \
   ${NVME_ARGS} \
   ${PCI_TESTDEV_ARGS} \
   ${PCI_HOSTDEV_ARGS} \
   -drive if=virtio,format=qcow2,file=${IMAGES}/${VM_NAME}.qcow2 \
   -netdev user,id=net0,hostfwd=tcp::${SSH_PORT}-:22 \
   -device virtio-net-pci,netdev=net0
